#!/usr/bin/env python2
# -*- coding: utf-8 -*-

import os
import sys
import signal
import yaml
import datetime
import csv
import re
import socket
import time
import argparse

from time import gmtime, strftime
from influxdb import InfluxDBClient
from threading import Event

# default values
DEFAULT_TIMEFORMAT="%Y-%m-%d %H:%M:%S"
DEFAULT_TIMEFIELD="datetime"
DEFAULT_BATCHSIZE=50
DEFAULT_DELAYBETWEENWRITES=200
DEFAULT_RETRYCOUNT=5
DEFAULT_RETRYINTERVAL=10000

# *** input parameters
confFile=sys.argv[1]

# *** global configuration
confDoc = open(confFile, 'r')
conf=yaml.load(confDoc)

# last time the db was accessed
lastdbcall_millis = 0

# exit event to correctly terminate the process
exit = Event()

# *** get the value of a property defined in path and returns an empty string if it does not exist
def conf_value(cfg, path, default='',delim='.'):
    finalDoc = cfg
    if finalDoc is not None:
        for e in path.split(delim):
        	try:
        		finalDoc = finalDoc[e]
        	except:
        		finalDoc = default;
    if finalDoc is None:
        finalDoc = default
    return finalDoc

# *** common parameters from the configuration
delay_writes   = conf_value(conf, "common.idbpusher.delay-between-writes", DEFAULT_DELAYBETWEENWRITES)
delay_runs     = conf_value(conf, "common.idbpusher.delay-between-runs", 10000)/1000
retry_count    = conf_value(conf, "common.idbpusher.connection-error-retry-count", DEFAULT_RETRYCOUNT)
retry_interval = conf_value(conf, "common.idbpusher.connection-error-retry-interval", DEFAULT_RETRYINTERVAL)/1000
max_batchsize  = conf_value(conf, "common.idbpusher.max_batchsize", DEFAULT_BATCHSIZE)

# *** helper Map object
class Map(dict):
    def __init__(self, *args, **kwargs):
        super(Map, self).__init__(*args, **kwargs)
        for arg in args:
            if isinstance(arg, dict):
                for k, v in arg.iteritems():
                    self[k] = v

        if kwargs:
            for k, v in kwargs.iteritems():
                self[k] = v

    def __getattr__(self, attr):
        return self.get(attr)

    def __setattr__(self, key, value):
        self.__setitem__(key, value)

    def __setitem__(self, key, value):
        super(Map, self).__setitem__(key, value)
        self.__dict__.update({key: value})

    def __delattr__(self, item):
        self.__delitem__(item)

    def __delitem__(self, key):
        super(Map, self).__delitem__(key)
        del self.__dict__[key]

# *** gets a lock using domain sockets to prevent this script from running more than once
def get_lock():
    get_lock._lock_socket = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
    try:
        get_lock._lock_socket.bind('\0idbpusher-magic-aw422d5522dft5saxg5')
        return True
    except socket.error:
        return False

# signal termination handler for graceful shutdown
def signal_term_handler(signal, frame):
    exit.set()

# *** unix time
epoch = datetime.datetime.utcfromtimestamp(0)
def unix_time_millis(dt):
    return int((dt - epoch).total_seconds() * 1000)

# *** configure the db client
def get_db_client():
    # db connection params
    db_url         = conf_value(conf, "common.idbpusher.db.url", None)
    db_user        = conf_value(conf, "common.idbpusher.db.user", None) 
    db_pass        = conf_value(conf, "common.idbpusher.db.pass", None)
    db_name        = conf_value(conf, "common.idbpusher.db.dbname", None)
    
    # check they have been defined
    if db_url == None or db_name == None:
        raise ValueError("Invalid DB connection details.")
    
    # parse url to get host and port
    m = re.search("http://([a-zA-Z0-9\.]+):([0-9]+)/?", db_url)    
    if not(m):
        raise ValueError('The DB url %s is invalid.'%db_url);
        
    # creat the client object
    client=InfluxDBClient(m.group(1), m.group(2), db_user, db_pass, db_name)
    return client

# *** get list of enabled umc ids from the definition file
def get_umc_ids():
    umcids=[]
    for key in conf:
        if conf_value(conf, key + ":enabled", False, ":") == True:
            m = re.match(r"^umc-(.+)",key) 
            if m:
                umcids.append(m.group(1))
    return umcids

# *** get umc id from the log file name
def get_umcid_from_logfile(logfile):
    dirname=os.path.dirname(logfile)
    m=re.match(".+/([a-zA-Z0-9\\._]+)/?$", dirname)
    if m:
        return m.group(1)
    else:
        sys.stderr.write("Cannot retrieve umc id from logfile " + logfile + "!\n")
        return None

# *** retrieves the list of all log files to be pushed to the influxdb sorted by modified time
def refresh_logs():
    pattern = re.compile(".+_[0-9]+.*\.log$")
    search_re=".*(" + "|".join(get_umc_ids()) + ")$";
    
    allfiles=[]
    for dirname, dirnames, filenames in os.walk("/home/vagrant/umc/logs"):
        if re.match(search_re, dirname):
            for filename in filenames:
                if pattern.match(filename):
                    allfiles.append(os.path.join(dirname, filename));
    return sorted(allfiles, key=lambda fn: os.stat(fn).st_mtime, reverse=True)
    
# *** convert to float, return None when cannot convert
def float_ex(val):
    try:
        return float(val)
    except:
        return None 

# *** evaluates filter on the row's tags and fields values        
def eval_filter(filter, tags, fields):
    try:
        for k,v in tags.items():
            if v is not None:
                exec(k + "=\"" + v + "\"")
        for k,v in fields.items():
            if v is not None:
                exec(k + "=" + str(v))
        return eval(filter)
    except Exception as e:
        sys.stderr.write("Error when evaluatin the filter '%s': %s!\n" % (filter, e)) 
        return False      

# *** reads and checks umc definition for a specific umc id
def get_umc_def(umc_id):
    # get umc definition
    umcconf=conf_value(conf, "umc-" + umc_id, None, ":")
    # check umc id and def were retrieved ok
    if umcconf is None:
        sys.stderr.write("Error when getting umc configuration for '%s'!\n" % (umc_id)) 
        return None

    # is enabled        
    enabled=conf_value(umcconf,"enabled",False)

    # get and check metric
    metric=conf_value(umcconf, "idbpusher.name", None)
        
    # tags and fields cols of this umc definition
    tcols = [x.strip().lower() for x in conf_value(umcconf, "idbpusher.tags").split(',') if x != '']
    fcols = [x.strip().lower() for x in conf_value(umcconf, "idbpusher.fields").split(',') if x != '']
    
    # combine with common tags and fields cols
    tcols.extend(x for x in [y.strip().lower() for y in conf_value(conf, "common.idbpusher.tags").split(',')] if x != '' and x not in tcols)
    fcols.extend(x for x in [y.strip().lower() for y in conf_value(conf, "common.idbpusher.fields").split(',')] if x != '' and x not in fcols)
    
    # read and check time field and its format
    timeformat=conf_value(umcconf, "idbpusher.timeformat", conf_value(conf, "common.idbpusher.timeformat", DEFAULT_TIMEFORMAT))
    try:
        if timeformat not in ['_unix_', '_time_s_', '_time_ms_']:
            strftime(timeformat, gmtime())
    except Exception as e:
        sys.stderr.write("The time format '%s' is invalid for umc '%s': %s!\n" % (timeformat,umc_id,e)) 
        return None
            
    timefield=conf_value(umcconf, "idbpusher.timefield", conf_value(conf, "common.idbpusher.timefield", DEFAULT_TIMEFIELD)).lower()
    filter=conf_value(umcconf, "idbpusher.filter", None)
    
    return Map(umcconf=umcconf,metric=metric,enabled=enabled,tcols=tcols,fcols=fcols,
        timeformat=timeformat,timefield=timefield,filter=filter,datapoints=[],datafiles=[])

# *** read data points from the csv log file
def read_datapoints(logfilename, umcdef):    
    datapoints = []
    if umcdef.enabled: 
        # read datapoints
        with open(logfilename, 'r') as csvfile:
            reader = csv.DictReader(csvfile, delimiter=',')
            for row in reader:
                # convert all keys to lowercase
                row = { k.lower():v for k, v in row.items() }
                
                # timestamp
                try:
                    if not(umcdef.timefield in row):
                        raise ValueError("Cannot find time field '" + umcdef.timefield + "' in data row!")                         
                    if umcdef.timeformat == "_unix_" or umcdef.timeformat == "_time_s_":
                        timestamp = long(row[umcdef.timefield]) * 1000000000  
                    elif umcdef.timeformat == "_time_ms_":
                        timestamp = long(row[umcdef.timefield]) * 1000000               
                    else:
                        timestamp = unix_time_millis(datetime.datetime.strptime(row[umcdef.timefield],umcdef.timeformat)) * 1000000
                except Exception as e:
                    # output error and skip this row
                    sys.stderr.write("Cannot read or convert time to timestamp: " + str(e) + "\n")
                    continue     
                
                # create tags and fields
                tags   = { k.lower():str(v)      for k, v in row.items() if k in umcdef.tcols }
                fields = { k.lower():float_ex(v) for k, v in row.items() if k in umcdef.fcols }
                
                # skip row and the filter is defined and holds on this row
                if umcdef.filter is not None and not(eval_filter(umcdef.filter, tags, fields)):
                    continue
                
                if len(fields) > 0:            
                    datapoints.append({ "measurement" : umcdef.metric, "time" : timestamp, "fields" : fields, "tags": tags })
            # // end reading rows
                
    return datapoints

# *** writes the data to the DB; retries when not successful
def db_write(client, datapoints, datafiles):    
    global lastdbcall_millis
    
    # wait between db writes
    millis=int(round(time.time() * 1000))
    if lastdbcall_millis > 0 and (millis - lastdbcall_millis < delay_writes):
        exit.wait((delay_writes - (millis - lastdbcall_millis))/1000)

    retryCount=0
    while not exit.is_set():
        retry = False                    
        try:
            # write the points and update the last db call time
            print "Writing datapoints for %s, len=%d..."%(umc_id,len(datapoints))
            response = client.write_points(datapoints)
            lastdbcall_millis = int(round(time.time() * 1000))
        except Exception as e:
            sys.stderr.write("Error occurred when inserting data: %s\n"%(e))                                                    
            if retryCount < retry_count:
                sys.stderr.write("Will retry in %d seconds...\n"%(retry_interval))                            
                exit.wait(retry_interval)
                retryCount=retryCount+1
                retry = True
            else:
                sys.stderr.write("Maximum number of %d retries reached!\n"%(retryCount))                          
                return False                            
            
        if not(retry):
            # success
            for file in datafiles:
                os.remove(file)
                pass
            break # break the retry loop
    # // end retry loop
    
    return True            
    
# *** MAIN

signal.signal(signal.SIGTERM, signal_term_handler)
signal.signal(signal.SIGINT, signal_term_handler)

# get the lock and exit when already running
if not(get_lock()):
    sys.exit(1)

umcdefs = {}

try:
    # get db client
    client=get_db_client()

    while not exit.is_set():
        # retrieve all log files
        alllogs=refresh_logs()

        while len(alllogs) > 0:
            logfile=alllogs.pop()
            
            # check if file exists
            if not(os.path.exists(logfile)):
                sys.stderr.write("The file " + logfile + " does not exist, is there another process working in logs directory?\n")
                continue
            
            # read and check umc definition for this file
            umc_id = get_umcid_from_logfile(logfile)        
            
            # skip this file if it cannot be identified
            if umc_id is None:
                continue
            
            # get the umc definition for this umc_id and store it    
            if umc_id not in umcdefs: 
                umcdef = get_umc_def(umc_id)
                if umcdef is None:
                    sys.stderr.write("The definition for umc id %s cannot be retrieved for file %s?\n" % (umc_id,logfile))
                    continue
                else:
                    umcdefs[umc_id] = umcdef
            
            # read datapoints from the log file and store them in the umcdef dict
            umcdefs[umc_id].datapoints += read_datapoints(logfile, umcdefs[umc_id])
            umcdefs[umc_id].datafiles.append(logfile)
            
            # write points in batches 
            if len(umcdefs[umc_id].datapoints) != 0 and len(umcdefs[umc_id].datapoints)/max_batchsize >= 1:
                if db_write(client, umcdefs[umc_id].datapoints, umcdefs[umc_id].datafiles):
                    umcdefs[umc_id].datapoints = []
                    umcdefs[umc_id].datafiles = []
                else:
                    # failed to write to db after many attempts
                    exit(1)
                        
        # // end log files iteration
        
        # flush the remaining write buffer
        for umc_id in umcdefs:
            if db_write(client, umcdefs[umc_id].datapoints, umcdefs[umc_id].datafiles):
                umcdefs[umc_id].datapoints = []
                umcdefs[umc_id].datafiles = []                
            else:
                # failed to write to db after many attempts
                exit(1)
                
        # check for changes of configuration file and realod here
        # todo
        
        # wait between runs
        exit.wait(delay_runs)
    
    # // end main loop
    
except Exception as e:
    print(e)
        

