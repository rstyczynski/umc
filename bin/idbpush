#!/usr/bin/env python2
# -*- coding: utf-8 -*-
#
# idbpush - a tool to push csv logs generated by umcrunner to influxdb
# 06-2018, Tomas Vitvar, tomas@vitvar.com

import os
import sys
import signal
import yaml
import datetime
import csv
import re
import socket
import time
import argparse

from time import gmtime, strftime
from influxdb import InfluxDBClient
from threading import Event

# default values
DEFAULT_TIMEFORMAT="%Y-%m-%d %H:%M:%S"
DEFAULT_TIMEFIELD="datetime"
DEFAULT_BATCHSIZE_ROWS=50
DEFAULT_BATCHSIZE_FILES=500
DEFAULT_DELAYBETWEENWRITES=200
DEFAULT_RETRYCOUNT=5
DEFAULT_RETRYINTERVAL=10000
DEFAULT_REMOVEFILESNODATA=True

# *** global variables
conf=None               # configuration file
lastdbcall = 0          # last time the db was accessed
exit = Event()          # exit event to correctly terminate the process
args=None               # input arguments
lasterror=None          # last error text
lasterrorcount=0        # last error count

# parameters to be read from the configuration file
delay_writes       = 0
delay_runs         = 0
retry_count        = 0
retry_interval     = 0
max_batchsize_rows = 0

# messages colors
class bcolors:
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    
# messages types
MSG_INFO  = 0
MSG_ERROR = 1
MSG_WARN  = 2
    
# *** print info message
def message(type, msg, forcePrint=False):
    global lasterror; global lasterrorcount
    
    # count number of errors and exit if the error message is the same as the previous one
    if type == MSG_ERROR and lasterror == msg:
        lasterrorcount=lasterrorcount+1
        return

    # default is stdout
    out = sys.stdout
    
    # error stream
    if type == MSG_ERROR or type == MSG_WARN:
        out = sys.stderr
        
    # colors
    out_color = ''; end_color_out = ''
    error_color=bcolors.FAIL; end_color_error = bcolors.ENDC
    if type == MSG_ERROR and sys.stderr.isatty():
        out_color = bcolors.FAIL; end_color_out = bcolors.ENDC
    if type == MSG_WARN and sys.stderr.isatty():
        out_color = bcolors.WARNING; end_color_out = bcolors.ENDC

    # only write info messages in verbose mode
    if ((args.verbose or forcePrint) and type == MSG_INFO) or type == MSG_ERROR or type == MSG_WARN:
        # the message has changed but tell how many previous error messages there were if not only one
        if lasterrorcount > 0:
            if not(sys.stderr.isatty()):
                error_color=''; end_color_error=''
                
            sys.stderr.write("[" + str(datetime.datetime.now()) + "]:" + error_color + 
                " The previous error occurred %d times!"%lasterrorcount + end_color_error + "\n")
            sys.stderr.flush()
            lasterrorcount=0
        
        # write the message to respective stream
        out.write("[" + str(datetime.datetime.now()) + "]:" + out_color + " " + msg + end_color_out + "\n")  
        out.flush()
        
        # if the message type is error, remember the message and reset the counter
        if type == MSG_ERROR:
            lasterror=msg
            lasterrorcount=0
        else:
            lasterror=None

def info1_msg(msg):
    message(MSG_INFO, msg, True)

def info2_msg(msg):
    message(MSG_INFO, msg)
    
def err_msg(msg):
    message(MSG_ERROR, msg)    

def warn_msg(msg):
    message(MSG_WARN, msg)    

# *** get the value of a property from yaml file defined by path and returns 
# an empty string if it does not exist
def conf_value(cfg, path, default='',delim='.'):
    finalDoc = cfg
    if finalDoc is not None:
        for e in path.split(delim):
        	try:
        		finalDoc = finalDoc[e]
        	except:
        		finalDoc = default;
    if finalDoc is None:
        finalDoc = default
    return finalDoc

# *** helper Map object
class Map(dict):
    def __init__(self, *args, **kwargs):
        super(Map, self).__init__(*args, **kwargs)
        for arg in args:
            if isinstance(arg, dict):
                for k, v in arg.iteritems():
                    self[k] = v

        if kwargs:
            for k, v in kwargs.iteritems():
                self[k] = v

    def __getattr__(self, attr):
        return self.get(attr)

    def __setattr__(self, key, value):
        self.__setitem__(key, value)

    def __setitem__(self, key, value):
        super(Map, self).__setitem__(key, value)
        self.__dict__.update({key: value})

    def __delattr__(self, item):
        self.__delitem__(item)

    def __delitem__(self, key):
        super(Map, self).__delitem__(key)
        del self.__dict__[key]

# *** gets a lock using domain sockets to prevent this script from running more than once
def get_lock():
    get_lock._lock_socket = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
    try:
        get_lock._lock_socket.bind('\0idbpush-magic-aw422d5522dft5saxg5')
        return True
    except socket.error:
        return False

# signal termination handler for graceful shutdown
def signal_quit(signal, frame):
    warn_msg("Shutting down...")
    exit.set()

# *** unix time
epoch = datetime.datetime.utcfromtimestamp(0)
def unix_time_millis(dt):
    return int((dt - epoch).total_seconds() * 1000)

# *** configure the db client
def get_db_client():
    # db connection params
    db_url         = conf_value(conf, "common.idbpush.db.url", None)
    db_user        = conf_value(conf, "common.idbpush.db.user", None) 
    db_pass        = conf_value(conf, "common.idbpush.db.pass", None)
    db_name        = conf_value(conf, "common.idbpush.db.dbname", None)
        
    # check the db was defined
    if db_url == None or db_name == None:
        raise ValueError("Invalid DB connection details (db_url or db_name is missing).")
    
    # parse url to get host and port
    m = re.search("http://([a-zA-Z0-9\.]+):([0-9]+)/?", db_url)    
    if not(m):
        raise ValueError('The DB url %s is invalid.'%db_url);
            
    # create the client object
    client=InfluxDBClient(m.group(1), m.group(2), db_user, db_pass, db_name)
    info1_msg("DB connection details are: host=%s,port=%s,user=%s,pass=xxxxx,dbname=%s"
        %(m.group(1),m.group(2),db_user,db_name))
    
    return client

# *** get list of enabled umc ids from the definition file
def get_umc_ids():
    umcids=[]
    for key in conf:
        if conf_value(conf, key + ":enabled", False, ":") == True:
            m = re.match(r"^umc-(.+)",key) 
            if m:
                umcids.append(m.group(1))
    return umcids

# *** get umc id from the log file name
def get_umcid_from_logfile(logfile):
    dirname=os.path.dirname(logfile)
    m=re.match(".+/([a-zA-Z0-9\\._]+)/?$", dirname)
    if m:
        return m.group(1)
    else:
        err_msg("Cannot retrieve umc id from logfile %s!"%logfile)
        return None

# *** retrieves the first batch of log files sorted by modified time
def get_batch_logs(batchsize=sys.maxint):
    pattern = re.compile(".+_[0-9]+.*\.log$")
    search_re=".*/(" + "|".join(get_umc_ids()) + ")$";
    
    batch=[]
    cnt=0
    for dirname, dirnames, filenames in os.walk(args.logs):
        if re.match(search_re, dirname):
            for filename in filenames:
                if pattern.match(filename):
                    cnt=cnt+1
                    if cnt <= batchsize: 
                        batch.append(os.path.join(dirname, filename))
        if cnt > batchsize:
            break
    return sorted(batch, key=lambda fn: os.stat(fn).st_mtime, reverse=True)
    
# *** convert to float, return None when cannot convert
def float_ex(val):
    try:
        return float(val)
    except:
        return None 

# *** evaluates filter on the row's tags and fields values        
def eval_filter(umc_id, filter, tags, fields):
    try:
        for k,v in tags.items():
            if v is not None:
                exec(k + "=\"" + v + "\"")
        for k,v in fields.items():
            if v is not None:
                exec(k + "=" + str(v))
        return eval(filter)
    except Exception as e:
        err_msg("Error when evaluating the filter '%s' for %s: %s!" % (filter, umc_id, str(e))) 
        return False      

def eval_transform(umc_id, transform, tags, fields):
    try:
        # declare variables and assign values to them
        for k,v in tags.items():
            if v is not None:
                exec(k + "=\"" + v + "\"")
        for k,v in fields.items():
            if v is not None:
                exec(k + "=" + str(v))
        
        # transform                 
        for e in transform:
            try:
                exec(e)
            except Exception as ex:
                info1_msg("Error when evaluating transformation '%s' for %s: %s"%(e,umc_id,str(ex)))

        # create resulting tags and fields
        __t2 = {}
        for k,v in tags.items():
            exec("__t2['%s']=%s"%(k,k))
        
        __f2 = {}
        for k,v in fields.items():
            exec("__f2['%s']=%s"%(k,k))
        
        return __t2,__f2
    except Exception as e:
        err_msg("Error when evaluating transformations for %s: %s"%(umc_id, str(e)))
        return tags,fields

# *** reads and checks umc definition for a specific umc id
def get_umc_def(umc_id):
    # get umc definition
    umcconf=conf_value(conf, "umc-" + umc_id, None, ":")
    # check umc id and def were retrieved ok
    if umcconf is None:
        err_msg("Error when getting umc configuration for '%s'!" % (umc_id)) 
        return None

    # is enabled        
    enabled=conf_value(umcconf,"enabled",False)

    # get and check metric
    metric=conf_value(umcconf, "idbpush.name", None)
        
    # tags and fields cols of this umc definition
    tcols = [x.strip() for x in conf_value(umcconf, "idbpush.tags").split(',') if x != '' ]
    fcols = [x.strip() for x in conf_value(umcconf, "idbpush.fields").split(',') if x != '' ]
    
    # combine with common tags and fields cols
    tcols.extend(x for x in 
        [y.strip() for y in conf_value(conf, "common.idbpush.tags").split(',') ] 
        if x != '' and x not in tcols and '!'+x not in tcols )
    fcols.extend(x for x in 
        [y.strip() for y in conf_value(conf, "common.idbpush.fields").split(',') ] 
        if x != '' and x not in fcols and '!'+x not in tcols )
    
    # remove all commented out fields and tags
    tcols = [x for x in tcols if not(x.startswith('!')) ]
    fcols = [x for x in fcols if not(x.startswith('!')) ]
    
    # read and check time field and its format
    timeformat=conf_value(umcconf, "idbpush.timeformat", conf_value(conf, "common.idbpush.timeformat", DEFAULT_TIMEFORMAT))
    try:
        if timeformat not in ['_unix_', '_time_s_', '_time_ms_']:
            strftime(timeformat, gmtime())
    except Exception as e:
        err_msg("The time format '%s' is invalid for umc '%s': %s!" % (timeformat,umc_id,e)) 
        return None
            
    timefield=conf_value(umcconf, "idbpush.timefield", conf_value(conf, "common.idbpush.timefield", DEFAULT_TIMEFIELD))   
    filter=conf_value(umcconf, "idbpush.filter", None)
    
    # transformation expressions
    transform=conf_value(umcconf, "idbpush.transform", None)
    
    return Map(umcid=umc_id,umcconf=umcconf,metric=metric,enabled=enabled,tcols=tcols,fcols=fcols,
        timeformat=timeformat,timefield=timefield,filter=filter,transform=transform,datapoints=[],datafiles=[])

# *** read data points from the csv log file
def read_datapoints(logfilename, umcdef):    
    datapoints = []; notags=False; nofields=False; 
    if umcdef.enabled and umcdef.metric is not None: 
        # read datapoints
        with open(logfilename, 'r') as csvfile:
            reader = csv.DictReader(csvfile, delimiter=',')
            for row in reader:
                # remove None keys
                row = { k:v for k, v in row.items() if k is not None }
                
                # timestamp
                try:
                    if not(umcdef.timefield in row):
                        raise ValueError("Cannot find time field '" + umcdef.timefield + "' in data row!")                         
                    if umcdef.timeformat == "_unix_" or umcdef.timeformat == "_time_s_":
                        timestamp = long(row[umcdef.timefield]) * 1000000000  
                    elif umcdef.timeformat == "_time_ms_":
                        timestamp = long(row[umcdef.timefield]) * 1000000               
                    else:
                        timestamp = unix_time_millis(datetime.datetime.strptime(row[umcdef.timefield],umcdef.timeformat)) * 1000000
                except Exception as e:
                    # output error and skip this row
                    err_msg("Cannot read or convert time to timestamp for %s: %s"%(umcdef.umcid,str(e)))
                    continue     
                
                # create tags and fields
                tags   = { k:str(v)      for k, v in row.items() if k in umcdef.tcols }
                fields = { k:float_ex(v) for k, v in row.items() if k in umcdef.fcols }                
                notags = (len(tags) == 0)

                # evaluate transformations
                if umcdef.transform is not None:
                    tags,fields = eval_transform(umcdef.umcid, umcdef.transform,tags,fields)

                # skip row if the filter is defined and holds on this row
                if umcdef.filter is not None and not(eval_filter(umcdef.umcid, umcdef.filter, tags, fields)):
                    continue
                                    
                if len(fields) > 0:            
                    datapoints.append({ "measurement" : umcdef.metric, "time" : timestamp, "fields" : fields, "tags": tags })
                    
            # // end reading rows
        # // end open file
            
    # check for no tags
    if notags and len(datapoints) > 0:
        warn_msg("The definition of %s contains no tags presented in the log file %s!"%(umcdef.umcid,os.path.basename(logfilename)))
                
    return datapoints

# *** writes the data to the DB; retries when not successful
def db_write(client, datapoints, datafiles):    
    global lastdbcall
    
    # wait between db writes
    millis=int(round(time.time() * 1000))
    if lastdbcall > 0 and (millis - lastdbcall < delay_writes):
        exit.wait((delay_writes - (millis - lastdbcall))/1000)

    retryCount=0
    while not exit.is_set():
        retry = False                    
        try:
            # write the points and update the last db call time
            response = client.write_points(datapoints)
            lastdbcall = int(round(time.time() * 1000))
            if retryCount > 0:
                info1_msg("Connection to the DB was successful after %d retries."%retryCount);
        except Exception as e:
            err_msg("Error occurred when inserting data: %s"%(e))                                                    
            if retry_count == -1 or retryCount < retry_count:
                err_msg("Will retry in %d seconds..."%(retry_interval))                            
                exit.wait(retry_interval)
                if exit.is_set():
                    return False
                retryCount=retryCount+1
                retry = True
            else:
                err_msg("Maximum number of %d retries reached!"%(retryCount))                          
                return False                            
            
        if not(retry):
            # success
            for file in datafiles:
                os.remove(file)
            break # break the retry loop
    # // end retry loop
    
    return True            
    
# *** MAIN
if __name__ == "__main__":    
    # arguments
    parser = argparse.ArgumentParser(description='InfluxDB push tool for umc')
    parser.add_argument('--runall', required=True, help='push a backlog of umc metrics defined in <file>',metavar='<file>')
    parser.add_argument('--logs', required=False, help='location of umc logs directory',metavar='<dir>')
    parser.add_argument('--verbose', required=False, help='be verbose',action='store_true')
    args=parser.parse_args()
    
    # get the lock and exit when already running
    if not(get_lock()):
        sys.exit(1)

    # register signals to quit the process
    for sig in ('TERM', 'HUP', 'INT'):
        signal.signal(getattr(signal, 'SIG'+sig), signal_quit);

    info1_msg("idbpush started.")

    # location of logs
    if args.logs == None:
        args.logs=os.environ.get('UMC_LOG_DIR',os.path.dirname(os.path.realpath(__file__)) + "/../logs") 

    info2_msg("Using configuration file %s"%args.runall)
    info2_msg("The logs directory is in %s"%args.logs)
    
    # read configuration file
    try:
        nfl=os.path.dirname(os.path.realpath(__file__)) + "/" + args.runall
        if not(os.path.exists(args.runall)) and os.path.exists(nfl):
            warn_msg("The file %s does not exist in the current directory, will try the idbpush home directory."%(args.runall))
            args.runall = nfl
        
        with open(args.runall, 'r') as confDoc:
            conf=yaml.load(confDoc)
        conf_mtime=os.stat(args.runall).st_mtime    
    except Exception as e:
        err_msg("Error when reading the configuration file %s: %s"%(args.runall,e))
        sys.exit(1)
    
    # configuration params
    delay_writes        = conf_value(conf, "common.idbpush.delay-between-writes", DEFAULT_DELAYBETWEENWRITES)
    delay_runs          = conf_value(conf, "common.idbpush.delay-between-runs", 10000)/1000
    retry_count         = conf_value(conf, "common.idbpush.connection-error-retry-count", DEFAULT_RETRYCOUNT)
    retry_interval      = conf_value(conf, "common.idbpush.connection-error-retry-interval", DEFAULT_RETRYINTERVAL)/1000
    max_batchsize_rows  = conf_value(conf, "common.idbpush.max-batchsize-rows", DEFAULT_BATCHSIZE_ROWS)
    max_batchsize_files = conf_value(conf, "common.idbpush.max-batchsize-files", DEFAULT_BATCHSIZE_FILES)

    umcdefs = {}

    try:
        # get db client
        client=get_db_client()

        while not exit.is_set():
            # retrieve logs in a batch of maximum 500 files
            batchlogs=get_batch_logs(max_batchsize_files)
            countRecords=0
            countFiles=0
            start_time = time.time()    
            info1_msg("There is %d files in the batch of logs."%(len(batchlogs)))

            # process all files in the batch
            while len(batchlogs) > 0:
                logfile=batchlogs.pop()
                
                # check if file exists
                if not(os.path.exists(logfile)):
                    err_msg("The file %s does not exist, is there another process working in logs directory?"%logfile)
                    continue
                
                # read and check umc definition for this file
                umc_id = get_umcid_from_logfile(logfile)        
                
                # skip this file if it cannot be identified
                if umc_id is None:
                    err_msg("Cannot determine umc_id from the log file %s, skipping this file."%logfile)                    
                    continue
                
                # get the umc definition for this umc_id and store it    
                if umc_id not in umcdefs: 
                    umcdef = get_umc_def(umc_id)
                    if umcdef is None:
                        err_msg("The definition for umc id %s cannot be retrieved for file %s?" % (umc_id,logfile))
                        continue
                    else:
                        info1_msg("Definition retrieved for umc %s"%(umc_id))
                        if not(umcdef.enabled):
                            info1_msg("umc id %s is disabled by configuration, no datapoints will be read."%(umc_id))
                        elif umcdef.metric is None:
                            info1_msg("umc id %s has no metric defined, no datapoints will be read."%(umc_id))                        
                        umcdefs[umc_id] = umcdef
                
                # read datapoints from the log file and store them in the umcdef dict
                umcdefs[umc_id].datapoints += read_datapoints(logfile, umcdefs[umc_id])
                umcdefs[umc_id].datafiles.append(logfile)
                info2_msg("Data points read for umc %s. There is currently %d records in %d files in the write buffer."%(umc_id, 
                    len(umcdefs[umc_id].datapoints), len(umcdefs[umc_id].datafiles)))        
                
                # write points in batches 
                if len(umcdefs[umc_id].datapoints) != 0 and len(umcdefs[umc_id].datapoints)/max_batchsize_rows >= 1:
                    if db_write(client, umcdefs[umc_id].datapoints, umcdefs[umc_id].datafiles):
                        info2_msg("Write buffer flushed for umc %s (%d records from %d files wirtten to the DB)."%(umc_id, 
                            len(umcdefs[umc_id].datapoints), len(umcdefs[umc_id].datafiles)))        
                        countRecords=countRecords+len(umcdefs[umc_id].datapoints)
                        countFiles=countFiles+len(umcdefs[umc_id].datafiles)
                        umcdefs[umc_id].datapoints = []
                        umcdefs[umc_id].datafiles = []
                    else:
                        # failed to write to the db after many attempts
                        err_msg("Force exit due to connection errors!")                    
                        sys.exit(1)
                        
            # // end log files iteration
            
            # flush the remaining write buffer
            for umc_id in umcdefs: 
                # check to remove files providing no data
                if len(umcdefs[umc_id].datapoints) == 0 and len(umcdefs[umc_id].datafiles) > 0:
                    warn_msg("All %d files in the batch of logs provide no data for %s!"%(len(umcdefs[umc_id].datafiles),umc_id))
                    if conf_value(conf, "common.idbpush.remove-backlog-files-no-data", False) == True:
                        warn_msg("%d files providing no data for %s will be removed!"%(len(umcdefs[umc_id].datafiles),umc_id))
                        for file in umcdefs[umc_id].datafiles:
                            os.remove(file)
                    umcdefs[umc_id].datafiles = []             
                elif len(umcdefs[umc_id].datapoints) > 0: 
                    # write remaining backlog from this iteration
                    if db_write(client, umcdefs[umc_id].datapoints, umcdefs[umc_id].datafiles):
                        info2_msg("Remaining write buffer flushed for umc %s (%d records from %d files wirtten to the DB)."%(umc_id, 
                            len(umcdefs[umc_id].datapoints), len(umcdefs[umc_id].datafiles)))        
                        countRecords=countRecords+len(umcdefs[umc_id].datapoints)
                        countFiles=countFiles+len(umcdefs[umc_id].datafiles)
                        umcdefs[umc_id].datapoints = []
                        umcdefs[umc_id].datafiles = []                
                    else:
                        # failed to write to db after many attempts
                        err_msg("Force exit due to connection errors!")                    
                        sys.exit(1)
                    
            # check for changes of configuration file
            if os.stat(args.runall).st_mtime != conf_mtime:
                if conf_value(conf, "common.idbpush.configuration-auto-reload", False) == True:
                    info1_msg("Configuration file %s has changed, will try to reload it."%args.runall)
                    try:
                        with open(args.runall, 'r') as confDoc2:
                            conf2=yaml.load(confDoc2)
                        umcdefs = {}                    
                        conf=conf2
                        info1_msg("The configuration was reloaed.")
                    except Exception as e:
                        info1_msg("Error occurred when realoding the configuration file:\n%s."%e)
                        info1_msg("Will use the previous configuration.")
                else:
                    warn_msg("Configuration file %s has changed but you need to restart idbpush manually for changes to take effect!"%args.runall)
                conf_mtime=os.stat(args.runall).st_mtime
            
            # display info message
            if countRecords > 0:
                info1_msg("Logs in the batch of logs cleared in %.2f seconds (%d files, %d records)"%
                    (time.time()-start_time,countFiles,countRecords))
                    
            # wait between runs
            info2_msg("Waiting %s seconds to start the next iteration."%(delay_runs))
            exit.wait(delay_runs)
            
        # // end main loop
        
        info2_msg("idbpush gracefully ended.")
        
    except Exception as e:
        err_msg("Failed due to error: %s"%e)
        

