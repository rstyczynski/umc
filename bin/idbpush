#!/usr/bin/env python2
# -*- coding: utf-8 -*-
#
# idbpush - a tool to push csv logs generated by umcrunner to influxdb
# 06-2018, Tomas Vitvar, tomas@vitvar.com

import os
import sys
import signal
import yaml
import datetime
import csv
import re
import socket
import time
import argparse
import messages as Msg
import utils

from time import gmtime, strftime
from influxdb import InfluxDBClient
from threading import Event
from utils import Map
from umcconfig import UmcConfig

from requests.exceptions import *

# global variables
epoch = datetime.datetime.utcfromtimestamp(0)

# global context
class GlobalContext():
    configFile=None
    confing=None            # configuration file
    lastdbcall = 0          # last time the db was accessed
    exit = Event()          # exit event to correctly terminate the process
    args=None               # input arguments
    lasterror=None          # last error text
    lasterrorcount=0        # last error count

# gets a lock using domain sockets to prevent this script from running more than once
def get_lock():
    get_lock._lock_socket = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
    try:
        get_lock._lock_socket.bind('\0idbpush-magic-aw422d5522dft5saxg5')
        return True
    except socket.error:
        return False

# signal termination handler for graceful shutdown
def signal_quit(signal, frame):
    Msg.warn_msg("Shutting down...")
    GlobalContext.exit.set()

# unix time
def unix_time_millis(dt):
    return int((dt - epoch).total_seconds() * 1000)

# retrieves the first batch of log files sorted by modified time
def get_batch_logs(GlobalContext):
    pattern = re.compile(".+_[0-9]+.*\.log.{log_file_group}$".format(log_file_group=GlobalContext.params.log_file_group))
    search_re=GlobalContext.args.logs + "/[a-zA-Z0-9\._\-]+/([a-zA-Z0-9\-\._]+)" # + "|".join(GlobalContext.config.umc_instanceids(False)) + ")$";
    umc_instanceids=GlobalContext.config.umc_instanceids(False)
    
    batch=[]; cnt=0
    for dirname, dirnames, filenames in os.walk(GlobalContext.args.logs):
        m=re.match(search_re, dirname)
        if m and m.group(1) in umc_instanceids:
            for filename in filenames:
                if pattern.match(filename):
                    cnt=cnt+1
                    if cnt <= GlobalContext.params.max_batchsize_files: 
                        batch.append(os.path.join(dirname, filename))
        if cnt > GlobalContext.params.max_batchsize_files:
            break
    return sorted(batch, key=lambda fn: os.stat(fn).st_mtime, reverse=True)
    
# evaluates filter on the row's tags and fields values        
def eval_filter(umc_id, filter, tags, fields):
    try:
        for k,v in tags.items():
            if v is not None:
                exec(k + "=\"" + v + "\"")
        for k,v in fields.items():
            if v is not None:
                exec(k + "=" + str(v))
        return eval(filter)
    except Exception as e:
        Msg.err_msg("Error when evaluating the filter '%s' for %s: %s!" % (filter, umc_id, str(e))) 
        return False      

def eval_transform(umc_id, transform, tags, fields):
    try:
        # declare variables and assign values to them
        for k,v in tags.items():
            if v is not None:
                exec(k + "=\"" + v + "\"")
        for k,v in fields.items():
            if v is not None:
                exec(k + "=" + str(v))
        
        # transform                 
        for e in transform:
            try:
                exec(e)
            except Exception as ex:
                pass
                #info1_msg("Error when evaluating transformation '%s' for %s: %s"%(e,umc_id,str(ex)))

        # create resulting tags and fields
        __t2 = {}
        for k,v in tags.items():
            exec("__t2['%s']=%s"%(k,k))
        
        __f2 = {}
        for k,v in fields.items():
            exec("__f2['%s']=%s"%(k,k))
        
        return __t2,__f2
    except Exception as e:
        Msg.err_msg("Error when evaluating transformations for %s: %s"%(umc_id, str(e)))
        return tags,fields

# read data points from the csv log file
def read_datapoints(GlobalContext, logfilename, umcdef):    
    datapoints = []; notags=False; nofields=False; 
    tzoffset = GlobalContext.params.tzoffset   
 
    if umcdef.enabled and umcdef.metric is not None: 
        # read datapoints
        with open(logfilename, 'r') as csvfile:
            reader = csv.DictReader(csvfile, delimiter=',')
            for row in reader:
                # remove None keys
                row = { k:v for k, v in row.items() if k is not None }
                
                # timestamp
                try:
                    if not(umcdef.timefield in row):
                        raise ValueError("Cannot find time field '" + umcdef.timefield + "' in data row!")                         
                    if umcdef.timeformat == "_unix_" or umcdef.timeformat == "_time_s_":
                        timestamp = long(row[umcdef.timefield]) * 1000000000  
                    elif umcdef.timeformat == "_time_ms_":
                        timestamp = long(row[umcdef.timefield]) * 1000000               
                    else:
                        if umcdef.tzfield is not None and umcdef.tzfield in row:
                            tzoffset = utils.float_ex(row[umcdef.tzfield], GlobalContext.params.tzoffset)                        
                        timestamp = (unix_time_millis(datetime.datetime.strptime(row[umcdef.timefield],umcdef.timeformat)) - int(tzoffset*60*60*1000)) * 1000000
                except Exception as e:
                    # output error and skip this row
                    Msg.err_msg("Cannot read or convert time to timestamp for %s: %s"%(umcdef.umcid,str(e)))
                    continue     
                
                # create tags and fields
                tags   = { k:str(v)      for k, v in row.items() if k in umcdef.tcols }
                fields = { k:utils.float_ex(v) for k, v in row.items() if k in umcdef.fcols }                
                notags = (len(tags) == 0)
                
                # only add this row if there is at least one field with some value
                if len([ v for k,v in fields.items() if v is not None ])>0:
                    # evaluate transformations
                    if umcdef.transform is not None:
                        tags,fields = eval_transform(umcdef.umcid, umcdef.transform,tags,fields)

                    # only add this row if filter holds on this row or there is no filter
                    if umcdef.filter is None or eval_filter(umcdef.umcid, umcdef.filter, tags, fields):
                        datapoints.append({ "measurement" : umcdef.metric, "time" : timestamp, "fields" : fields, "tags": tags })
                    
            # // end reading rows
        # // end open file
            
    # check for no tags
    if notags and len(datapoints) > 0:
        Msg.warn_msg("The definition of %s contains no tags presented in the log file %s!"%(umcdef.umcid,os.path.basename(logfilename)))
                
    return datapoints

# writes the data to the DB; retries when not successful
def db_write(GlobalContext, datapoints, datafiles):    
    # wait between db writes
    millis=int(round(time.time() * 1000))
    if GlobalContext.lastdbcall > 0 and (millis - GlobalContext.lastdbcall < GlobalContext.params.delay_writes):
        GlobalContext.exit.wait((GlobalContext.params.delay_writes - (millis - GlobalContext.lastdbcall))/1000)

    retryCount=0
    while not GlobalContext.exit.is_set():
        retry = False                    
        try:
            # write the points and update the last db call time
            response = GlobalContext.client.write_points(datapoints)
            GlobalContext.lastdbcall = int(round(time.time() * 1000))
            if retryCount > 0:
                Msg.info1_msg("Connection to the DB was successful after %d retries."%retryCount);
        except (ConnectionError, Timeout) as e:
            Msg.err_msg("Error occurred when inserting data: %s"%(e))                                                    
            if GlobalContext.params.retry_count == -1 or retryCount < GlobalContext.params.retry_count:
                Msg.err_msg("Will retry in %d seconds..."%(GlobalContext.params.retry_interval))                            
                GlobalContext.exit.wait(GlobalContext.params.retry_interval)
                if GlobalContext.exit.is_set():
                    return False
                retryCount=retryCount+1
                retry = True
            else:
                Msg.err_msg("Maximum number of %d retries reached!"%(retryCount))                          
                return False                                            
        except Exception as e:
            # TODO: check the status code and if 400, do something with the data in the buffer
            # such as dump it to a text file or try to divide it to parts and submit it
            Msg.err_msg("Error occurred when inserting data, all records in the write buffer (%d) will be discarded!: %s"
                %(len(datapoints),str(e)))
            retry = False
            pass                                                    
            
        if not(retry):
            # success
            for file in datafiles:
                os.remove(file)
            break # break the retry loop
    # // end retry loop
    
    return True            
    
# *** MAIN
if __name__ == "__main__":    
    # arguments
    parser = argparse.ArgumentParser(description='InfluxDB push tool for umc')
    parser.add_argument('--config', required=False, help='push a backlog of umc metrics defined in <file>',metavar='<file>')
    parser.add_argument('--logs', required=False, help='location of umc logs directory',metavar='<dir>')
    parser.add_argument('--verbose', required=False, help='be verbose',action='store_true')
    GlobalContext.args=parser.parse_args()
    Msg.verbose=GlobalContext.args.verbose
    
    # get the lock and exit when already running
    if not(get_lock()):
        sys.exit(1)

    # register signals to quit the process
    for sig in ('TERM', 'HUP', 'INT'):
        signal.signal(getattr(signal, 'SIG'+sig), signal_quit);

    Msg.info1_msg("idbpush started.")

    # location of logs
    if GlobalContext.args.logs == None:
        GlobalContext.args.logs=os.environ.get('UMC_LOG_DIR',os.path.dirname(os.path.realpath(__file__)) + "/../logs") 
    
    # configuration file
    GlobalContext.configFile=GlobalContext.args.config
    if GlobalContext.configFile == None:
        GlobalContext.configFile=os.environ.get('UMCRUNNER_CONFIG', None)
    if GlobalContext.configFile is None:
        sys.stderr.write("Configuration file must be specified on command line or in UMCRUNNER_CONFIG environment variable!\n")
        sys.exit(1)
    
    GlobalContext.config=UmcConfig(GlobalContext.configFile)
    
    Msg.info2_msg("Using configuration file %s"%GlobalContext.configFile)
    Msg.info2_msg("The logs directory is in %s"%GlobalContext.args.logs)

    GlobalContext.params = GlobalContext.config.idbpush_params() 

    umcdefs = {}

    try:
        # get db client
        GlobalContext.client=InfluxDBClient(GlobalContext.params.host, GlobalContext.params.port, GlobalContext.params.db_user, 
            GlobalContext.params.db_pass, GlobalContext.params.db_name)
        Msg.info1_msg("DB connection details are: host=%s,port=%s,user=%s,pass=xxxxx,dbname=%s"
            %(GlobalContext.params.host,GlobalContext.params.port,GlobalContext.params.db_user,GlobalContext.params.db_name))

        while not GlobalContext.exit.is_set():
            # retrieve logs in a batch of maximum 500 files
            batchlogs=get_batch_logs(GlobalContext)
            Msg.info1_msg("There is %d files in the batch of logs."%(len(batchlogs)))
            
            # process all files in the batch
            countRecords=0; countFiles=0; 
            start_time = time.time()    
            while len(batchlogs) > 0:
                logfile=batchlogs.pop()
                
                # check if file exists
                if not(os.path.exists(logfile)):
                    Msg.err_msg("The file %s does not exist, is there another process working in logs directory?"%logfile)
                    continue
                
                # read and check umc definition for this file
                umc_id = GlobalContext.config.get_umcid_from_logfile(logfile)        
                
                # skip this file if it cannot be identified
                if umc_id is None:
                    Msg.err_msg("Cannot determine umc_id from the log file %s, skipping this file."%logfile)                    
                    continue
                
                # get the umc definition for this umc_id and store it    
                if umc_id not in umcdefs: 
                    try: 
                        umcdef = GlobalContext.config.idbpush_umcdef(umc_id)
                        
                        Msg.info1_msg("Definition retrieved for umc %s"%(umc_id))
                        if not(umcdef.enabled):
                            Msg.info1_msg("umc id %s is disabled by configuration, no datapoints will be read."%(umc_id))
                        elif umcdef.metric is None:
                            Msg.info1_msg("umc id %s has no metric defined, no datapoints will be read."%(umc_id))                        
                        umcdefs[umc_id] = umcdef
                        
                    except Exception as e:
                        Msg.err_msg("The definition for umc id %s cannot be retrieved from file %s due to %s" % (umc_id,logfile,str(e)))
                        continue
                
                # read datapoints from the log file and store them in the umcdef dict
                umcdefs[umc_id].datapoints += read_datapoints(GlobalContext,logfile, umcdefs[umc_id])
                umcdefs[umc_id].datafiles.append(logfile)
                Msg.info2_msg("Data points read for umc %s. There is currently %d records in %d files in the write buffer."%(umc_id, 
                    len(umcdefs[umc_id].datapoints), len(umcdefs[umc_id].datafiles)))        
                
                # write points in batches 
                if len(umcdefs[umc_id].datapoints) != 0 and len(umcdefs[umc_id].datapoints)/GlobalContext.params.max_batchsize_rows >= 1:
                    if db_write(GlobalContext, umcdefs[umc_id].datapoints, umcdefs[umc_id].datafiles):
                        Msg.info2_msg("Write buffer flushed for umc %s (%d records from %d files wirtten to the DB)."%(umc_id, 
                            len(umcdefs[umc_id].datapoints), len(umcdefs[umc_id].datafiles)))        
                        countRecords=countRecords+len(umcdefs[umc_id].datapoints)
                        countFiles=countFiles+len(umcdefs[umc_id].datafiles)
                        umcdefs[umc_id].datapoints = []
                        umcdefs[umc_id].datafiles = []
                    else:
                        # failed to write to the db after many attempts
                        Msg.err_msg("Force exit due to connection errors!")                    
                        sys.exit(1)
                        
            # // end log files iteration
            
            # flush the remaining write buffer
            for umc_id in umcdefs: 
                # check to remove files providing no data
                if len(umcdefs[umc_id].datapoints) == 0 and len(umcdefs[umc_id].datafiles) > 0:
                    Msg.warn_msg("All %d files in the batch of logs provide no data for %s!"%(len(umcdefs[umc_id].datafiles),umc_id))
                    if GlobalContext.config.value("common.idbpush.remove-backlog-files-no-data", False) == True:
                        Msg.warn_msg("%d files providing no data for %s will be removed!"%(len(umcdefs[umc_id].datafiles),umc_id))
                        for file in umcdefs[umc_id].datafiles:
                            os.remove(file)
                    umcdefs[umc_id].datafiles = []             
                elif len(umcdefs[umc_id].datapoints) > 0: 
                    # write remaining backlog from this iteration
                    if db_write(GlobalContext, umcdefs[umc_id].datapoints, umcdefs[umc_id].datafiles):
                        Msg.info2_msg("Remaining write buffer flushed for umc %s (%d records from %d files wirtten to the DB)."%(umc_id, 
                            len(umcdefs[umc_id].datapoints), len(umcdefs[umc_id].datafiles)))        
                        countRecords=countRecords+len(umcdefs[umc_id].datapoints)
                        countFiles=countFiles+len(umcdefs[umc_id].datafiles)
                        umcdefs[umc_id].datapoints = []
                        umcdefs[umc_id].datafiles = []                
                    else:
                        # failed to write to db after many attempts
                        Msg.err_msg("Force exit due to connection errors!")                    
                        sys.exit(1)
                    
            if countRecords > 0:
                Msg.info1_msg("Logs in the batch of logs cleared in %.2f seconds (%d files, %d records)"%
                    (time.time()-start_time,countFiles,countRecords))
                    
            # wait between runs
            Msg.info2_msg("Waiting %s seconds to start the next iteration."%(GlobalContext.params.delay_runs))
            GlobalContext.exit.wait(GlobalContext.params.delay_runs)
            
        # // end main loop
        
        Msg.info2_msg("idbpush gracefully ended.")
        
    except Exception as e:
        Msg.err_msg("Failed due to error: %s"%e)
        

