#!/usr/bin/env python2
# -*- coding: utf-8 -*-
#
# umcpush daemon - a tool to push data from csv logs generated by umcrunner to various destinations such as influxdb or OMC
# 06-2018, Tomas Vitvar, tomas@vitvar.com

import os
import sys
import signal
import yaml
import datetime
import csv
import re
import socket
import time
import argparse
import messages as Msg
import utils
import umcwriter

from time import gmtime, strftime
from threading import Event
from utils import Map
from umcconfig import UmcConfig
from umcreader import UmcReader
from umcwriter import UmcWriter

from requests.exceptions import *

# global context
class GlobalContext():
    id=None                 # id of the push tool
    configFile=None         # configuration file
    confing=None            # UmcConfig object
    lastwritecall = 0          # last time the db was accessed
    exit = Event()          # exit event to correctly terminate the process
    lasterror=None          # last error text
    lasterrorcount=0        # last error count

# data buffer class
class Buffer(object):
    def __init__(self):
        self.countRecords=0
        self.countFiles=0
        self.datapoints=[]
        self.datafiles=[]
    
    def reset(self, counters=False):
        self.countRecords+=len(self.datapoints)
        self.countFiles+=len(self.datafiles)
        self.datapoints=[]
        self.datafiles=[]
        
        if counters:
            self.countRecords=0
            self.countFiles=0
    # // reset
# // Buffer

# gets a lock using domain sockets to prevent this script from running more than once
def get_lock(id):
    get_lock._lock_socket = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
    try:
        get_lock._lock_socket.bind('\0umcpush-magic-aw422d5522dft5saxg5_' + id)
        return True
    except socket.error:
        return False
# // get_lock

# signal termination handler for graceful shutdown
def signal_quit(signal, frame):
    Msg.warn_msg("Shutting down...")
    GlobalContext.exit.set()

# writes the data to the destination; retries when not successful
def write(GlobalContext, datapoints, datafiles):    
    # wait between db writes
    millis=int(round(time.time() * 1000))
    if GlobalContext.lastwritecall > 0 and (millis - GlobalContext.lastwritecall < GlobalContext.writer.params.delay_writes):
        GlobalContext.exit.wait((GlobalContext.writer.params.delay_writes - (millis - GlobalContext.lastwritecall))/1000)

    retryCount=0
    while not GlobalContext.exit.is_set():
        retry = False                    
        try:
            # write the points and update the last db call time
            response = GlobalContext.writer.write(datapoints, GlobalContext.exit)
            GlobalContext.lastwritecall = int(round(time.time() * 1000))
            if retryCount > 0:
                Msg.info1_msg("The connection to the writer's destination was successful after %d retries."%retryCount);
        except (ConnectionError, Timeout) as e:
            Msg.err_msg("Error occurred when inserting data: %s"%(e))                                                    
            if GlobalContext.writer.params.connection_retry_count == -1 or retryCount < GlobalContext.writer.params.connection_retry_count:
                Msg.err_msg("Will retry in %d seconds..."%(GlobalContext.writer.params.connection_retry_interval))                            
                GlobalContext.exit.wait(GlobalContext.writer.params.connection_retry_interval)
                if GlobalContext.exit.is_set():
                    return False
                retryCount=retryCount+1
                retry = True
            else:
                # this will make the umcpush exit
                raise Exception("Maximum number of %d retries reached!"%(retryCount))                          
        except Exception as e:
            Msg.err_msg("Error occurred when inserting data, all records in the write buffer (%d) will be discarded!: %s"
                %(len(datapoints),str(e)))
            retry = False
            pass                                                    
            
        if not(retry):
            for file in datafiles:
                os.remove(file)
            break # break the retry loop
    # // end retry loop
    
    return True            
    
# *** MAIN
if __name__ == "__main__":    
    # arguments
    parser = argparse.ArgumentParser(description="push csv logs files to a writer's destination generated by umcrunner")
    parser.add_argument('--writer', required=True, help='a writer classname.')
    parser.add_argument('--config', required=False, help='configuration file <file>',metavar='<file>')
    parser.add_argument('--logs', required=False, help='location of umc logs directory',metavar='<dir>')
    parser.add_argument('--verbose', required=False, help='be verbose',action='store_true')
    args=parser.parse_args()
    Msg.verbose=args.verbose
    
    # get the lock and exit when already running
    if not(get_lock(args.writer)):
        sys.exit(1)

    # register signals to quit the process
    for sig in ('TERM', 'HUP', 'INT'):
        signal.signal(getattr(signal, 'SIG'+sig), signal_quit);

    try:
        # tool id welcome
        GlobalContext.writer_class=args.writer
        Msg.info1_msg("umcpush started with writer class '%s'."%GlobalContext.writer_class)

        # create the main configuration object
        GlobalContext.config=UmcConfig(GlobalContext.configFile, args.logs)
        Msg.info2_msg("Using configuration file %s"%GlobalContext.config.configFile)
        Msg.info2_msg("The logs directory is in %s"%GlobalContext.config.logDir)

        # storage of umc definitions from the configuration file
        umcdefs = {}
        
        # write buffer 
        buffer=Buffer()
    
        # instantiate reader and writer objects
        GlobalContext.writer=umcwriter.create_instance(GlobalContext.config, GlobalContext.writer_class)        
        GlobalContext.reader=UmcReader(GlobalContext.config, GlobalContext.writer.writer_id)

        # main umcpush tool
        while not GlobalContext.exit.is_set():
            # retrieve logs in a batch of maximum NUM files
            batchlogs=GlobalContext.reader.get_batch_logs(GlobalContext.config.logDir)
            Msg.info1_msg("A batch of %d log files loaded."%(len(batchlogs)))
            
            # process all files in the batch
            start_time = time.time()    
            buffer.reset(counters=True)
            while len(batchlogs) > 0 and not GlobalContext.exit.is_set():
                logfile=batchlogs.pop()
                
                # check if the file exists
                if not(os.path.exists(logfile)):
                    Msg.err_msg("The file %s does not exist, is there another process working in logs directory?"%logfile)
                    continue
                
                # read and check umc definition for this file
                umc_id = GlobalContext.config.get_umcid_from_logfile(logfile)        
                if umc_id is None:
                    Msg.err_msg("Cannot determine umc_id from the log file %s, skipping this file."%logfile)                    
                    continue
                
                # get the umc definition for this umc_id and store it    
                if umc_id not in umcdefs: 
                    # get umc configuration
                    umcconf=GlobalContext.config.read_umcdef(umc_id)
                    if umcconf is None:
                        Msg.err_msg("The umc definition with id %s file %s does not exist in the configuration file, skipping the file"
                            %(umc_id,logfile))
                        continue
                    else:
                        umcdef=Map(umcid=umc_id,enabled=False,writer=None,reader=None)
                        umcdef.enabled = GlobalContext.config.value_element(umcconf,"enabled",False)                        
                        umcdef.writer=GlobalContext.writer.read_umcdef(umc_id,umcconf)
                        umcdef.reader=GlobalContext.reader.read_umcdef(umc_id,umcconf)
                        Msg.info1_msg("Definition retrieved for umc %s"%(umc_id))
                        
                        if not(umcdef.enabled):
                            Msg.info1_msg("umc id %s is disabled by configuration, no datapoints will be read."%(umc_id))
                        elif umcdef.writer is None or umcdef.reader is None:
                            Msg.info2_msg("umc id %s does not have reader or writer definitions, will disable it."%(umc_id))
                            umcdef.enabled=False
    
                        # disable if the writer is not enabled
                        if not(umcdef.writer.enabled):
                            Msg.info2_msg("umc id %s writer is disabled."%(umc_id))                            
                            umcdef.enabled = False
                        
                        umcdefs[umc_id] = umcdef                        
                    # // else
                # // umc_id create 
                
                # process this umcdef if enabled
                if umcdefs[umc_id].enabled: 
                    # read datapoints from the log file and store them in the umcdef dict
                    buffer.datapoints += GlobalContext.reader.read_datapoints(logfile,umcdefs[umc_id], GlobalContext.writer.createWriteItem)
                    buffer.datafiles.append(logfile)
                    Msg.info2_msg("Data points read for umc %s. There is currently %d records in %d files in the write buffer."
                        %(umc_id,len(buffer.datapoints),len(buffer.datafiles)))        
                    
                    # write points in batches of max_batchsize_rows
                    if len(buffer.datapoints) != 0 and len(buffer.datapoints)/GlobalContext.reader.params.max_batchsize_rows >= 1:
                        write(GlobalContext, buffer.datapoints, buffer.datafiles)
                        Msg.info2_msg("Write buffer flushed (%d records from %d files writen out)."
                            %(len(buffer.datapoints),len(buffer.datafiles)))  
                        buffer.reset()      
                    # // batch write
                # // enabled
            # // end log files iteration
            
            if not GlobalContext.exit.is_set():
                # flush the remaining write buffer
                if len(buffer.datapoints) > 0: 
                    write(GlobalContext, buffer.datapoints, buffer.datafiles)
                    Msg.info2_msg("Remaining write buffer flushed (%d records from %d files writen out)."
                        %(len(buffer.datapoints), len(buffer.datafiles)))  
                    buffer.reset()      
                # len datapoints > 0
                
                if buffer.countRecords > 0:
                    Msg.info1_msg("Logs in the batch of logs cleared in %.2f seconds (%d files, %d records)"%
                        (time.time()-start_time,buffer.countFiles,buffer.countRecords))
                        
                # wait between runs
                Msg.info2_msg("Waiting %s seconds to start the next iteration."%(GlobalContext.writer.params.delay_runs))
                GlobalContext.exit.wait(GlobalContext.writer.params.delay_runs)
            # // if not exit
            
        # // end main loop
        
        Msg.info1_msg("umcpush gracefully ended.")
        
    except Exception as e:
        Msg.err_msg("Failed due to error: %s"%e)
        raise

