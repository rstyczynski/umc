#!/usr/bin/env python2
# -*- coding: utf-8 -*-
#
# umcpush daemon - a tool to push data from csv logs generated by umcrunner to various destinations such as influxdb or OMC
# 06-2018, Tomas Vitvar, tomas@vitvar.com

import os
import sys
import signal
import yaml
import datetime
import csv
import re
import socket
import time
import argparse
import messages as Msg
import utils

from time import gmtime, strftime
from threading import Event
from utils import Map
from umcconfig import UmcConfig
from umcreader import UmcReader

from requests.exceptions import *

# global context
class GlobalContext():
    id=None                 # id of the push tool
    configFile=None         # configuration file
    confing=None            # UmcConfig object
    lastwritecall = 0          # last time the db was accessed
    exit = Event()          # exit event to correctly terminate the process
    lasterror=None          # last error text
    lasterrorcount=0        # last error count

# gets a lock using domain sockets to prevent this script from running more than once
def get_lock(id):
    get_lock._lock_socket = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
    try:
        get_lock._lock_socket.bind('\0umcpush-magic-aw422d5522dft5saxg5_' + id)
        return True
    except socket.error:
        return False
# // get_lock

def getModuleClass(config,id):
    # create writer and reader
    key="common.umcpush.{id}.class".format(id=GlobalContext.id)
    cn=config.value(key)
    if cn is None or cn=="":
        raise Exception("The classname does not exist in %s."%key)

    mc=cn.split(".")
    if len(mc)==2:
        return Map(module=mc[0], classname=mc[1])
    else:
        raise Exception("The class definition %s is not required format \{module\}.\{classname\}!"%key)
# // getModuleClass

# signal termination handler for graceful shutdown
def signal_quit(signal, frame):
    Msg.warn_msg("Shutting down...")
    GlobalContext.exit.set()

# writes the data to the destination; retries when not successful
def write(GlobalContext, datapoints, datafiles):    
    # wait between db writes
    millis=int(round(time.time() * 1000))
    if GlobalContext.lastwritecall > 0 and (millis - GlobalContext.lastwritecall < GlobalContext.writer.params.delay_writes):
        GlobalContext.exit.wait((GlobalContext.writer.params.delay_writes - (millis - GlobalContext.lastwritecall))/1000)

    retryCount=0
    while not GlobalContext.exit.is_set():
        retry = False                    
        try:
            # write the points and update the last db call time
            response = GlobalContext.writer.write(datapoints)
            GlobalContext.lastwritecall = int(round(time.time() * 1000))
            if retryCount > 0:
                Msg.info1_msg("Connection to the DB was successful after %d retries."%retryCount);
        except (ConnectionError, Timeout) as e:
            Msg.err_msg("Error occurred when inserting data: %s"%(e))                                                    
            if GlobalContext.writer.params.retry_count == -1 or retryCount < GlobalContext.writer.params.retry_count:
                Msg.err_msg("Will retry in %d seconds..."%(GlobalContext.writer.params.retry_interval))                            
                GlobalContext.exit.wait(GlobalContext.writer.params.retry_interval)
                if GlobalContext.exit.is_set():
                    return False
                retryCount=retryCount+1
                retry = True
            else:
                Msg.err_msg("Maximum number of %d retries reached!"%(retryCount))                          
                return False                                            
        except Exception as e:
            # TODO: check the status code and if 400, do something with the data in the buffer
            # such as dump it to a text file or try to divide it to parts and submit it
            Msg.err_msg("Error occurred when inserting data, all records in the write buffer (%d) will be discarded!: %s"
                %(len(datapoints),str(e)))
            retry = False
            pass                                                    
            
        if not(retry):
            # success
            for file in datafiles:
                os.remove(file)
            break # break the retry loop
    # // end retry loop
    
    return True            
    
# *** MAIN
if __name__ == "__main__":    
    # arguments
    parser = argparse.ArgumentParser(description='umcpush - a tool to push csv logs files generated by umc instances to a destination')
    parser.add_argument('--run', required=True, help='Id defining a configuration to run. The configuration must exist in the config file.')
    parser.add_argument('--config', required=False, help='push a backlog of umc metrics defined in <file>',metavar='<file>')
    parser.add_argument('--logs', required=False, help='location of umc logs directory',metavar='<dir>')
    parser.add_argument('--verbose', required=False, help='be verbose',action='store_true')
    args=parser.parse_args()
    Msg.verbose=args.verbose
    
    # get the lock and exit when already running
    if not(get_lock(args.run)):
        sys.exit(1)

    # register signals to quit the process
    for sig in ('TERM', 'HUP', 'INT'):
        signal.signal(getattr(signal, 'SIG'+sig), signal_quit);

    try:
        # tool id welcome
        GlobalContext.id=args.run
        Msg.info1_msg("umcpush started, will use %s writer."%GlobalContext.id)

        # create the main configuration object
        GlobalContext.config=UmcConfig(GlobalContext.configFile, args.logs)
        Msg.info2_msg("Using configuration file %s"%GlobalContext.config.configFile)
        Msg.info2_msg("The logs directory is in %s"%GlobalContext.config.logDir)

        # storage of umc definitions from the configuration file
        umcdefs = {}
    
        # instantiate reader and writer objects
        # writer object is instantiated dynamically based on the class in the configuration file
        cdef=getModuleClass(GlobalContext.config,GlobalContext.id)
        class_ = getattr(__import__(cdef.module), cdef.classname)
        GlobalContext.writer=class_(GlobalContext.config, GlobalContext.id)        
        GlobalContext.reader=UmcReader(GlobalContext.config, GlobalContext.id)

        # main umcpush tool
        while not GlobalContext.exit.is_set():
            # retrieve logs in a batch of maximum NUM files
            batchlogs=GlobalContext.reader.get_batch_logs(GlobalContext.config.logDir)
            Msg.info1_msg("There is %d files in the batch of logs."%(len(batchlogs)))
            
            # process all files in the batch
            countRecords=0; countFiles=0; 
            start_time = time.time()    
            while len(batchlogs) > 0:
                logfile=batchlogs.pop()
                
                # check if file exists
                if not(os.path.exists(logfile)):
                    Msg.err_msg("The file %s does not exist, is there another process working in logs directory?"%logfile)
                    continue
                
                # read and check umc definition for this file
                umc_id = GlobalContext.config.get_umcid_from_logfile(logfile)        
                if umc_id is None:
                    Msg.err_msg("Cannot determine umc_id from the log file %s, skipping this file."%logfile)                    
                    continue
                
                # get the umc definition for this umc_id and store it    
                if umc_id not in umcdefs: 
                    # get umc configuration
                    umcconf=GlobalContext.config.read_umcdef(umc_id)
                    if umcconf is None:
                        Msg.err_msg("The umc definition with id %s file %s does not exist in the configuration file, skipping the file"
                            %(umc_id,logfile))
                        continue
                    else:
                        umcdef=Map(umcid=umc_id,enabled=False,writer=None,reader=None,datapoints=[],datafiles=[])
                        umcdef.enabled = GlobalContext.config.value_element(umcconf,"enabled",False)                        
                        umcdef.writer=GlobalContext.writer.read_umcdef(umc_id,umcconf)
                        umcdef.reader=GlobalContext.reader.read_umcdef(umc_id,umcconf)
                        Msg.info1_msg("Definition retrieved for umc %s"%(umc_id))
                        
                        if not(umcdef.enabled):
                            Msg.info1_msg("umc id %s is disabled by configuration, no datapoints will be read."%(umc_id))
                        umcdefs[umc_id] = umcdef                        
                    # // else
                # // umc_id create 
                
                # read datapoints from the log file and store them in the umcdef dict
                umcdefs[umc_id].datapoints += GlobalContext.reader.read_datapoints(logfile,umcdefs[umc_id], GlobalContext.writer.createWriteItem)
                umcdefs[umc_id].datafiles.append(logfile)
                Msg.info2_msg("Data points read for umc %s. There is currently %d records in %d files in the write buffer."%(umc_id, 
                    len(umcdefs[umc_id].datapoints), len(umcdefs[umc_id].datafiles)))        
                
                # write points in batches of max_batchsize_rows
                if len(umcdefs[umc_id].datapoints) != 0 and len(umcdefs[umc_id].datapoints)/GlobalContext.reader.params.max_batchsize_rows >= 1:
                    if write(GlobalContext, umcdefs[umc_id].datapoints, umcdefs[umc_id].datafiles):
                        Msg.info2_msg("Write buffer flushed for umc %s (%d records from %d files wirtten out)."%(umc_id, 
                            len(umcdefs[umc_id].datapoints), len(umcdefs[umc_id].datafiles)))        
                        countRecords=countRecords+len(umcdefs[umc_id].datapoints)
                        countFiles=countFiles+len(umcdefs[umc_id].datafiles)
                        umcdefs[umc_id].datapoints = []
                        umcdefs[umc_id].datafiles = []
                    else:
                        # failed to write to the db after many attempts
                        raise Exception("Force exit due to connection errors!")                    
                # // batch write
            # // end log files iteration
            
            # flush the remaining write buffer
            for umc_id in umcdefs: 
                # check to remove files providing no data
                if len(umcdefs[umc_id].datapoints) == 0 and len(umcdefs[umc_id].datafiles) > 0:
                    Msg.warn_msg("All %d files in the batch of logs provide no data for %s!"%(len(umcdefs[umc_id].datafiles),umc_id))
                    if GlobalContext.writer.params.remove_nodata_files == True:
                        Msg.warn_msg("%d files providing no data for %s will be removed!"%(len(umcdefs[umc_id].datafiles),umc_id))
                        for file in umcdefs[umc_id].datafiles:
                            os.remove(file)
                    umcdefs[umc_id].datafiles = []             
                elif len(umcdefs[umc_id].datapoints) > 0: 
                    # write remaining backlog from this iteration
                    if write(GlobalContext, umcdefs[umc_id].datapoints, umcdefs[umc_id].datafiles):
                        Msg.info2_msg("Remaining write buffer flushed for umc %s (%d records from %d files writen out)."%(umc_id, 
                            len(umcdefs[umc_id].datapoints), len(umcdefs[umc_id].datafiles)))        
                        countRecords=countRecords+len(umcdefs[umc_id].datapoints)
                        countFiles=countFiles+len(umcdefs[umc_id].datafiles)
                        umcdefs[umc_id].datapoints = []
                        umcdefs[umc_id].datafiles = []                
                    else:
                        # failed to write to db after many attempts
                        raise Exception("Force exit due to connection errors!")  
            # // remaining buffer                  
                    
            if countRecords > 0:
                Msg.info1_msg("Logs in the batch of logs cleared in %.2f seconds (%d files, %d records)"%
                    (time.time()-start_time,countFiles,countRecords))
                    
            # wait between runs
            Msg.info2_msg("Waiting %s seconds to start the next iteration."%(GlobalContext.writer.params.delay_runs))
            GlobalContext.exit.wait(GlobalContext.writer.params.delay_runs)
            
        # // end main loop
        
        Msg.info2_msg("umcpush gracefully ended.")
        
    except Exception as e:
        Msg.err_msg("Failed due to error: %s"%e)
        raise
        

