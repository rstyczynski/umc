#!/bin/bash
# access logs 

getSensorData_bin=$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )

args=("$@")

# the first parameter is timestamp directive
# since we do not use it, we skip it (the value is None)
shift

# the next arguments are delay and count
delay=$1 && shift
count=$1 && shift
config=$1 && shift

# seconds in a minute when the script will wake up to collect data in every iteration
# this is only used when delay=-1
wakeup_secs_in_minute=20

# check config file exists
if [ ! -f "$getSensorData_bin/$config" ]; then
	echo >&2 "$(date): The configuration file '$getSensorData/$config' does not exist!" 
	exit 1
fi

# load configuration
source $getSensorData_bin/$config

# sensor directory
getSensorData_bin=$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )

# header
echo "time,system,server,url,errors,success,timeAvg_ms,timeP95_ms,timeP90_ms,sizeAvg_bytes"

current_count=0
while [ $current_count -lt $count ]; do
	current_count=$[$current_count+1]

	# current time since epoch
	currenttime=$(date +%s); 
	
	# search all log files
	find "$accesslogs_root" 2>/dev/null | egrep "$accesslog_file$" | \
	while read log; do

		# server that this access log is coming from
		if [ "$server_pattern" != "" ]; then 
			server=$(echo $log | egrep -o $server_pattern)
		fi
		if [ "$server" = "" ]; then server="n/a"; fi

		# create the filter 
		# time to retrieve the rows from the access log
		# this is the last completed minute; calculated using epoch time
		e=$(date +%s); 
		
		#debug
		#e=$(date --date "2018-07-25 12:23:05" +%s);
		#e=$(date --date "2018-07-27 09:48:02" +%s);
		
		n=$(expr $e / 60); r=$(expr $n \* 60 - 60); 
		filter=$(date -d @$r +"$al_filter")
		csvtimestamp=$(date -d @$r +"$al_csvtimestampformat")
		
		# get the data from the log for further aggregation
		lines=$(cat $log | egrep "$filter"; echo magic324) 
		lines=${lines%magic324}

		# get all urls in this minute that match HTTP method filter and do not match url exclude filter
		echo "$lines" | awk -v mf=$al_methodfield -v mfilter="$al_methodfilter" -v uex="$al_urlexclude" -v uf=$al_urlfield \
			'{ if (match($mf,mfilter) && (uex == "" || !match($uf,uex))) print $uf }' | sort | uniq | \
			while read url; do
				# lines with this url only
				urllines=$(echo "$lines" | awk -v uf=$al_urlfield -v url=$url '{ if ($uf==url) print $0 }')

				# count success and errors
			  nerrs=$(echo "$urllines" | awk -v f=$al_statusfield '{ if (match($f,"4|5[0-9]+")) n++ } END { print n?n:0 }')
				nsucc=$(echo "$urllines" | awk -v f=$al_statusfield '{ if (match($f,"2[0-9]+")) n++ } END { print n?n:0 }')

				# calc average and p95 processing times
				if [ $al_proctimefield -gt 0 ]; then
			  	avgpt=$(echo "$urllines" | awk -v ptf=$al_proctimefield '{ n++;sum+=$ptf} END { printf "%.0f",n?(sum/n)*1000:0 }')
					percpt=$(echo "$urllines" | awk -v ptf=$al_proctimefield '{ print $ptf }' | sort -n | awk '{ values[NR] = $0*1000} END { printf "%.0f,%.0f",
						values[int(NR*0.95-0.5)],values[int(NR*0.90-0.5)] }')
				else
					avgpt="0"
					percpt="0,0"
				fi

				# calc average size	
				if [ $al_bytesfield -gt 0 ]; then			
					avgbt=$(echo "$urllines" | awk -v bf=$al_bytesfield '{ n++;sum+=$bf} END { printf "%.0f",n?(sum/n):0 }')
				else
					avgbt=0
				fi
				
				# print output
			  echo $csvtimestamp,$HOSTNAME,$server,$url,$nerrs,$nsucc,$avgpt,$percpt,$avgbt
						  
			done

	done

	# when delay = -1, then calc the next iteration wake up time
	# otherwise sleep delay seconds
	if [ $current_count -lt $count ]; then
		if [ $delay -eq -1 ]; then
			n=$(expr $currenttime / 60); ni=$(expr $n \* 60 + 60 + $wakeup_secs_in_minute); e=$(date +%s);
			sleep $(expr $ni - $e)
		else
			sleep $delay
		fi
	fi
	
done
